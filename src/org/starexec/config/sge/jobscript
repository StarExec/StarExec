#!/bin/bash
#==========================================================================
# Standard SGE job script setup.
#==========================================================================

# The queue to submit to
#$ -q $$QUEUE$$
#OAR -q $$QUEUE$$
#OAR -p queue='$$QUEUE$$'

# Enable resource limit signals
#$ -notify

# Submit under sandbox user 
#$ -u sandbox

# Resource limits
#$ -l s_fsize=$$MAX_WRITE$$G 

# we are using runsolver now instead of giving these to SGE: -l s_vmem=$$MAX_MEM$$G -l s_rt=$$MAX_RUNTIME$$ -l s_cpu=$$MAX_CPUTIME$$

# Default shell is bash
#$ -S /bin/bash

# Merge stdout and stderr streams
#$ -j y

export BENCH_NAME_LENGTH_LIMIT='$$BENCH_NAME_LENGTH_MAX$$'
export PRIMARY_PREPROCESSOR_PATH='$$PRIMARY_PREPROCESSOR_PATH$$'
export RAND_SEED='$$RANDSEED$$'
export MAX_MEM='$$MAX_MEM$$'
export BENCH_SAVE_DIR='$$BENCH_SAVE_PATH$$'
export USER_ID='$$USERID$$'
export HAS_DEPENDS='$$HAS_DEPENDS$$'
export BENCH_PATH='$$BENCH$$'
export PAIR_ID='$$PAIRID$$'
export STAREXEC_MAX_WRITE='$$MAX_WRITE$$'
export STAREXEC_CPU_LIMIT='$$MAX_CPUTIME$$'
export STAREXEC_WALLCLOCK_LIMIT='$$MAX_RUNTIME$$'
export REPORT_HOST='$$REPORT_HOST$$'
export SHARED_DIR='$$STAREXEC_DATA_DIR$$'
export SPACE_PATH='$$SPACE_PATH$$'
export SCRIPT_PATH='$$SCRIPT_PATH$$'
export PAIR_OUTPUT_DIRECTORY='$$PAIR_OUTPUT_DIRECTORY$$'
export DB_NAME='$$DB_NAME$$'
export BUILD_JOB='$$BUILD_JOB$$'
export WORKING_DIR_BASE='$$WORKING_DIR_BASE$$'
export BENCH_ID='$$BENCH_ID$$'
# Path to Olivier Roussel's runSolver
export RUNSOLVER_PATH='$$RUNSOLVER_PATH$$'
# Include common functions and status codes
. /home/starexec/sge_scripts/functions.bash


# Array of secondary benchmarks starexec paths
declare -a BENCH_DEPENDS_ARRAY

# Array of secondary benchmarks execution host paths
declare -a LOCAL_DEPENDS_ARRAY

#==========================================================================
# Arrays of stage information written from Java
#
#
#==========================================================================

$$STAGE_NUMBER_ARRAY$$

$$CPU_TIMEOUT_ARRAY$$

$$CLOCK_TIMEOUT_ARRAY$$

$$MEM_LIMIT_ARRAY$$

$$SOLVER_ID_ARRAY$$

$$SOLVER_TIMESTAMP_ARRAY$$

$$CONFIG_NAME_ARRAY$$

$$PRE_PROCESSOR_PATH_ARRAY$$

$$POST_PROCESSOR_PATH_ARRAY$$

$$SPACE_ID_ARRAY$$

$$SOLVER_NAME_ARRAY$$

$$SOLVER_PATH_ARRAY$$

$$BENCH_INPUT_ARRAY$$

$$BENCH_SUFFIX_ARRAY$$

$$RESULTS_INTERVAL_ARRAY$$

$$STDOUT_SAVE_OPTION_ARRAY$$

$$EXTRA_SAVE_OPTION_ARRAY$$

STAGE_INDEX=0

NUM_STAGES=${#STAGE_NUMBERS[@]}

NUM_BENCH_INPUTS=$((${#BENCH_INPUT_PATHS[@]}-1)) # we terminate the array with an empty string, so the number is the size-1

#==========================================================================
# Signal Traps
#
# These are just for the limits we are asking SGE to enforce.
# For the ones runsolver is enforcing, we will look in the
# watcher.out file.
#==========================================================================

trap "limitExceeded 'file write' $EXCEED_FILE_WRITE" SIGXFSZ


handleUsrTwo() {
	
  echo "Jobscript received USR2."
}

trap 'handleUsrTwo' USR2

#==========================================================================
# Execute (prolog will take care of data staging)

whoami
echo "STAREXEC Job Log"
echo "(C) `date +%Y` The University of Iowa"
echo "====================================="
echo "date: `date`"
echo "user: $USER_ID"
echo "pair id: $PAIR_ID"
echo "execution host: $HOSTNAME"
if [ $BUILD_JOB == "true" ]; then
        echo "This is a solver build job"
fi

echo ""

sendStatus $STATUS_RUNNING

initSandbox


if ! isInteger $SANDBOX ; then
        sendStatus $ERROR_RUNSCRIPT
        sendStatusToLaterStages $ERROR_RUNSCRIPT 0

        log "unable to secure any sandbox for this job!"
        exit 0
fi

if [ $SANDBOX -eq -1 ]
then
        sendStatus $ERROR_RUNSCRIPT
        sendStatusToLaterStages $ERROR_RUNSCRIPT 0

        log "unable to secure any sandbox for this job!"
        exit 0
fi


sendNode "$HOSTNAME" "$SANDBOX"
cleanWorkspace 1
setStartTime

createLocalTmpDirectory

log "there are this many benchmark inputs $NUM_BENCH_INPUTS"

decodePathArrays

$$STAGE_DEPENDENCY_ARRAY$$


# setup the memory limit for this stage

#Gets the memory of the node in kilobytes
NODE_MEM=$(vmstat -s | head -1 | sed 's/K total memory//')

#then, convert kb to mb
NODE_MEM=$(($NODE_MEM/1024))

log "node memory in megabytes = $NODE_MEM"

#then, set to half the memory
NODE_MEM=$(($NODE_MEM/2))

if [ $MAX_MEM -gt $NODE_MEM ]
then

echo "truncating max memory from requested $MAX_MEM to $NODE_MEM (half max memory on the node)"

export MAX_MEM=$NODE_MEM

fi

fillDependArrays

#todo: alter the way we copy runsolver over so it only happens once
copyBenchmarkDependencies

TMP=`mktemp --tmpdir=$TMPDIR starexec_base64.XXXXXXXX`
log "the temp directory is $TMP"
echo $PAIR_OUTPUT_DIRECTORY > $TMP
PAIR_OUTPUT_DIRECTORY=`base64 -d $TMP`
#==============================================================================
# This is the start of the loop that runs for each stage
#
#
#==============================================================================


# directory containing all the output files from a solver. This won't exist until the second stage
# and onwards
OTHER_INPUT_PATH=""

# if we are using pipelines, the OTHER_INPUT_PATH should always exist.
# so, we start with an empty directory here
if [ $NUM_STAGES -gt 1 ] 
then
	mkdir "$OUT_DIR"/empty_output_dir
	OTHER_INPUT_PATH="$OUT_DIR"/empty_output_dir
fi

log "the number of stages is $NUM_STAGES"

while [ $STAGE_INDEX -lt $NUM_STAGES ]
do


CURRENT_STAGE_MEMORY=$MAX_MEM   #start with the default max mem
CURRENT_STAGE_CPU=$STAREXEC_CPU_LIMIT
CURRENT_STAGE_WALLCLOCK=$STAREXEC_WALLCLOCK_LIMIT

if [ $CURRENT_STAGE_MEMORY -gt ${STAGE_MEM_LIMITS[$STAGE_INDEX]} ]
then

CURRENT_STAGE_MEMORY=${STAGE_MEM_LIMITS[$STAGE_INDEX]}

fi

if [ $CURRENT_STAGE_CPU -gt ${STAGE_CPU_TIMEOUTS[$STAGE_INDEX]} ]
then

	CURRENT_STAGE_CPU=${STAGE_CPU_TIMEOUTS[$STAGE_INDEX]}

fi

if [ $CURRENT_STAGE_WALLCLOCK -gt ${STAGE_CLOCK_TIMEOUTS[$STAGE_INDEX]} ]
then

	CURRENT_STAGE_WALLCLOCK=${STAGE_CLOCK_TIMEOUTS[$STAGE_INDEX]}

fi
POST_PROCESSOR_PATH=${POST_PROCESSOR_PATHS[$STAGE_INDEX]}
PRE_PROCESSOR_PATH=${PRE_PROCESSOR_PATHS[$STAGE_INDEX]}

CONFIG_NAME=${CONFIG_NAMES[$STAGE_INDEX]}
SOLVER_NAME=${SOLVER_NAMES[$STAGE_INDEX]}
SOLVER_PATH=${SOLVER_PATHS[$STAGE_INDEX]}
SOLVER_ID=${SOLVER_IDS[$STAGE_INDEX]}
SOLVER_TIMESTAMP=${SOLVER_TIMESTAMPS[$STAGE_INDEX]}
SPACE_ID=${SPACE_IDS[$STAGE_INDEX]}
ARGUMENT_STRING=${STAGE_DEPENDENCIES[$STAGE_INDEX]}
CURRENT_BENCH_SUFFIX=${BENCH_SUFFIXES[$STAGE_INDEX]}
SUPPRESS_TIMESTAMP=$$SUPPRESS_TIMESTAMP_OPTION$$
RESULTS_INTERVAL=${RESULTS_INTERVALS[$STAGE_INDEX]}
STDOUT_SAVE_OPTION=${STDOUT_SAVE_OPTIONS[$STAGE_INDEX]}
EXTRA_SAVE_OPTION=${EXTRA_SAVE_OPTIONS[$STAGE_INDEX]}

#path to where cached solvers are stored
SOLVER_CACHE_PATH="$WORKING_DIR_BASE/solvercache/$SOLVER_TIMESTAMP/$SOLVER_ID"

#whether the solver was found in the cache
SOLVER_CACHED=0

if [ $BUILD_JOB == "true" ]; then
    LOCAL_CONFIG_PATH="$LOCAL_SOLVER_DIR/starexec_build"
else
# The path to the config run script on the execution host
    LOCAL_CONFIG_PATH="$LOCAL_SOLVER_DIR/bin/starexec_run_$CONFIG_NAME"
fi

createDir "$OUT_DIR/output_files"


checkCache
copyDependencies
verifyWorkspace
sandboxWorkspace


CURRENT_STAGE_NUMBER=${STAGE_NUMBERS[$STAGE_INDEX]}

if [ "$SUPPRESS_TIMESTAMP" = true ] ; then
	log "Suppressing timestamp."
else
	log "Not suppressing timestamp."
fi

SANDBOX_FOUND=true
# SANDBOX_PARAM designates which user will run runsolver
if [ $SANDBOX -eq 1 ] ; then
	SANDBOX_PARAM="$$SANDBOX_USER_ONE$$"
	CORES="0-3"
elif [ $SANDBOX -eq 2 ] ; then
	SANDBOX_PARAM="$$SANDBOX_USER_TWO$$"
	CORES="4-7"
else
	SANDBOX_FOUND=false
fi

if [ "$SUPPRESS_TIMESTAMP" = true ] ; then
  TIMESTAMP_PARAM=""
  ADDEOF_PARAM=""
else
  TIMESTAMP_PARAM="--timestamp"
  ADDEOF_PARAM="--add-eof"
fi

#Give job pairs 2 extra minutes before we kill them.
JOB_PAIR_EXTRA_TIME=120

log "doing stage index $STAGE_INDEX"
log "stage number : $CURRENT_STAGE_NUMBER"
log "cpu timeout : $CURRENT_STAGE_CPU"
log "clock timeout : $CURRENT_STAGE_WALLCLOCK"
log "global cpu timeout : $STAREXEC_CPU_LIMIT"
log "global wallclock timeout : $STAREXEC_WALLCLOCK_LIMIT"
log "extra time for job pair after wallclock timeout : $JOB_PAIR_EXTRA_TIME"
log "mem limit : $CURRENT_STAGE_MEMORY"
log "solver id : $SOLVER_ID"
log "solver name : $SOLVER_NAME"
log "solver path : $SOLVER_PATH"
log "solver timestamp : $SOLVER_TIMESTAMP"
log "config name : $CONFIG_NAME"
log "pre processor path : $PRE_PROCESSOR_PATH"
log "post processor path : $POST_PROCESSOR_PATH"
log "space id : $SPACE_ID"
log "bench path : $BENCH_PATH"
log "benchmark suffix : $CURRENT_BENCH_SUFFIX"
log "argument string : $ARGUMENT_STRING"
log "results interval : $RESULTS_INTERVAL"
log "stdout save option : $STDOUT_SAVE_OPTION"
log "additional output save option : $EXTRA_SAVE_OPTION"


if [ "$SANDBOX_FOUND" = true ] ; then
	echo "Checking who current user is using: whoami ..."	
	whoami

	# this will be set to true in killDeadlockedJobPair if the job pair gets deadlocked
	echo "Calling killDeadlockedJobPair"

	killDeadlockedJobPair $CURRENT_STAGE_WALLCLOCK $JOB_PAIR_EXTRA_TIME $SANDBOX_PARAM &
	# Get the process of our defensive killDeadlockedJobPair function.
	KILL_DEADLOCKED_JOB_PAIR_PID=$!
	echo "KILL_DEADLOCKED_JOB_PAIR_PID = $KILL_DEADLOCKED_JOB_PAIR_PID"
	if [ $RESULTS_INTERVAL -gt 0 ] ; then
		copyOutputIncrementally $RESULTS_INTERVAL $STAREXEC_WALLCLOCK_LIMIT $CURRENT_STAGE_NUMBER $STDOUT_SAVE_OPTION $EXTRA_SAVE_OPTION &
		COPY_OUTPUT_INCREMENTALLY_PID=$!
	fi
    if [ $BUILD_JOB == "true" ]; then
        cd $WORKING_DIR_BASE/$SANDBOX_PARAM/solver
        rm $LOCAL_BENCH_PATH
        LOCAL_BENCH_PATH=""
    else
	    cd $WORKING_DIR_BASE/$SANDBOX_PARAM/solver/bin
    fi
	pwd

	log "listing contents of the output directory"
	ls -l $OUT_DIR
	# run runsolver
	echo "  $LOCAL_RUNSOLVER_PATH $TIMESTAMP_PARAM $ADDEOF_PARAM --cores $CORES -C $$MAX_CPUTIME$$ -W $$MAX_RUNTIME$$ -M $$MAX_MEM$$  -w $OUT_DIR/watcher.out -v $OUT_DIR/var.out -o $OUT_DIR/stdout.txt $LOCAL_CONFIG_PATH $LOCAL_BENCH_PATH"
	sudo -u $SANDBOX_PARAM TMPDIR=$LOCAL_TMP_DIR STAREXEC_MAX_MEM=$CURRENT_STAGE_MEMORY STAREXEC_MAX_WRITE=$$MAX_WRITE$$ STAREXEC_CPU_LIMIT=$CURRENT_STAGE_CPU STAREXEC_WALLCLOCK_LIMIT=$CURRENT_STAGE_WALLCLOCK "$LOCAL_RUNSOLVER_PATH" $TIMESTAMP_PARAM $ADDEOF_PARAM --cores $CORES -C $$MAX_CPUTIME$$ -W $$MAX_RUNTIME$$ -M $MAX_MEM  -w "$OUT_DIR/watcher.out" -v "$OUT_DIR/var.out" -o "$OUT_DIR/stdout.txt" "$LOCAL_CONFIG_PATH" "$LOCAL_BENCH_PATH" "$OUT_DIR/output_files" $OTHER_INPUT_PATH $ARGUMENT_STRING

	# move the working directory back to the top level sandbox
	cd $WORKING_DIR_BASE/$SANDBOX_PARAM
	if [ $RESULTS_INTERVAL -gt 0 ] ; then
		kill $COPY_OUTPUT_INCREMENTALLY_PID
	fi

	# If runsolver finishes then stop the killDeadlockedJobPair function.
	log "Runsolver finished. Killing the defensive killall function."
	kill $KILL_DEADLOCKED_JOB_PAIR_PID
	KILL_EXIT_STATUS=$?
	JOB_PAIR_DEADLOCKED=false
	if [ $KILL_EXIT_STATUS = 1 ]; then
		#killDeadlockedJobPair was dead so the job pair
		log "Job pair was deadlocked."
		JOB_PAIR_DEADLOCKED=true
	fi
else
	log "no job run for pair $PAIR_ID because a sandbox was not found"
	sendStatus $ERROR_RUNSCRIPT
fi

log "listing contents of the output directory"
ls -l $OUT_DIR

log "recording header of job pair output"
head -5 "$OUT_DIR/stdout.txt"

# runsolver dumps a lot of information to the WATCHFILE, and summary of times and such to VARFILE
WATCHFILE="$OUT_DIR"/watcher.out
VARFILE="$OUT_DIR"/var.out
STDOUT_FILE="$OUT_DIR"/stdout.txt

if [ ! -f $WATCHFILE ]; then
    log "Runsolver watchfile could not be found. Terminating job pair."
    markRunscriptError ${STAGE_NUMBERS[$STAGE_INDEX]}
    exit 0
fi

if [ ! -f $VARFILE ]; then
    echo "Runsolver varfile could not be found. Terminating job pair."
	markRunscriptError ${STAGE_NUMBERS[$STAGE_INDEX]}
    exit 0
fi

if ! isOutputValid $STDOUT_FILE $CURRENT_STAGE_NUMBER $SUPPRESS_TIMESTAMP ; then
	log "runsolver output was not valid. Terminating job pair."
	markRunscriptError ${STAGE_NUMBERS[$STAGE_INDEX]}
	exit 0
fi

# check for which cores we are on
log "epilog checking for information on cores, from runsolver's watch file:"
grep 'cores:' $WATCHFILE

# this also does post processing
copyOutput $CURRENT_STAGE_NUMBER $STDOUT_SAVE_OPTION $EXTRA_SAVE_OPTION

if [ $STDOUT_SAVE_OPTION -eq 3 ]; then
	saveStdoutAsBenchmark
fi

if [ $EXTRA_SAVE_OPTION -eq 3 ]; then
	saveExtraOutputAsBenchmarks
fi

# cleared below if no error
JOB_ERROR="1";


if ( grep 'job error:' "$SGE_STDOUT_PATH" ) then
  true 
elif ( grep 'wall clock time exceeded' $WATCHFILE ) || [ "$JOB_PAIR_DEADLOCKED" = true ]; then
  log "epilog detects wall clock time exceeded"
  sendStatus $EXCEED_RUNTIME
  sendStageStatus $EXCEED_RUNTIME ${STAGE_NUMBERS[$STAGE_INDEX]}
  break
  
elif ( grep 'CPU time exceeded' $WATCHFILE ) then
  log "epilog detects cpu time exceeded"
  sendStatus $EXCEED_CPU
  sendStageStatus $EXCEED_CPU ${STAGE_NUMBERS[$STAGE_INDEX]}
  break
  
  
elif ( grep 'VSize exceeded' $WATCHFILE ) then
  log "epilog detects max virtual memory exceeded"
  sendStatus $EXCEED_MEM
  sendStageStatus $EXCEED_MEM ${STAGE_NUMBERS[$STAGE_INDEX]}
  break
  
else
  JOB_ERROR="";
  log "execution on $HOSTNAME complete"
  sendStageStatus $STATUS_COMPLETE ${STAGE_NUMBERS[$STAGE_INDEX]}
  
  if [ $STAGE_INDEX -eq $(($NUM_STAGES-1)) ]
  then
  	  log "job finished successfully"
  	  sendStatus $STATUS_COMPLETE
  fi  
fi

if [ $(getWallclockTimeFromVarfile $1) -gt $CURRENT_STAGE_WALLCLOCK]; then
	log "job pair timed out based on wallclock time, but runsolver did not indicate this!"
	sendStatus $EXCEED_RUNTIME
  	sendStageStatus $EXCEED_RUNTIME ${STAGE_NUMBERS[$STAGE_INDEX]}
  	JOB_ERROR="1"
  	log "copying watchfile to log"
  	cat $WATCHFILE
fi

if [ $(getCpuTimeFromVarfile $1) -gt $CURRENT_STAGE_CPU]; then
	log "job pair timed out based on cpu time, but runsolver did not indicate this!"
	sendStatus $EXCEED_CPU
  	sendStageStatus $EXCEED_CPU ${STAGE_NUMBERS[$STAGE_INDEX]}
  	JOB_ERROR="1"
  	log "copying watchfile to log"
  	cat $WATCHFILE
fi

if [ "$JOB_ERROR" = "" ]; then
	log "STAREXEC job $PAIR_ID completed successfully"	
else
	log "STAREXEC job $PAIR_ID exited with errors"
	sendStatusToLaterStages $STATUS_NOT_REACHED ${STAGE_NUMBERS[$STAGE_INDEX]}
  	setRunStatsToZeroForLaterStages ${STAGE_NUMBERS[$STAGE_INDEX]}
  	cleanUpAfterKilledBuildJob
fi


if [ $BUILD_JOB == "true" ]; then
    copySolverBack
fi

cleanForNextStage


# update the path to the input to the output of the last stage
BENCH_PATH="$SAVED_OUTPUT_DIR/$CURRENT_STAGE_NUMBER"
OTHER_INPUT_PATH=$SAVED_OUTPUT_DIR"/"$CURRENT_STAGE_NUMBER"_output"

STAGE_INDEX=$(($STAGE_INDEX+1))

done

setEndTime

recordJobPairRun

cleanWorkspace 0 $SANDBOX_PARAM

echo "Jobscript ending."

